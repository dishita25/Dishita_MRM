{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:42:55.451613Z","iopub.execute_input":"2025-03-28T13:42:55.451887Z","iopub.status.idle":"2025-03-28T13:42:56.355340Z","shell.execute_reply.started":"2025-03-28T13:42:55.451860Z","shell.execute_reply":"2025-03-28T13:42:56.354469Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom transformers import get_cosine_schedule_with_warmup, AdamW\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:42:56.356241Z","iopub.execute_input":"2025-03-28T13:42:56.356720Z","iopub.status.idle":"2025-03-28T13:43:03.153330Z","shell.execute_reply.started":"2025-03-28T13:42:56.356680Z","shell.execute_reply":"2025-03-28T13:43:03.152658Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load WikiText-103 dataset\ndataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:43:03.154074Z","iopub.execute_input":"2025-03-28T13:43:03.154501Z","iopub.status.idle":"2025-03-28T13:43:11.253210Z","shell.execute_reply.started":"2025-03-28T13:43:03.154479Z","shell.execute_reply":"2025-03-28T13:43:11.252591Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d41d0300094cdbab65533f73390a7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/722k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c575249979374c14b8935f75f68316e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c770079ac25644c898b632970475740d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dbdd28d3d2e4b88baa9d5689827a4b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/655k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2605b730ddd34f48882e1922a28e1caf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb03235b6c9b40c0a4e388cf30f62b26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dcb3de95fa54e2897dfb2f1c2f35a4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bce8d1ce3ce4b3c91a8b5f3de87a448"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Initialize RoBERTa tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n\n# Remove empty or whitespace-only entries\ndataset = dataset.filter(\n    lambda x: x[\"text\"] is not None and len(x[\"text\"].strip()) > 0\n)\n\n# Tokenization function with MLM masking\ndef tokenize_function(examples):\n    inputs = tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        max_length=128,\n        padding=\"max_length\",\n        return_tensors=\"pt\",\n    )\n\n    labels = inputs[\"input_ids\"].clone()  # Copy input_ids for labels\n\n    rand = torch.rand(labels.shape)\n    mask_token_id = tokenizer.mask_token_id\n    vocab_size = tokenizer.vocab_size\n\n    mask_arr = (rand < 0.15) & (labels != tokenizer.pad_token_id) & (labels != tokenizer.cls_token_id) & (labels != tokenizer.sep_token_id)\n\n    # 80%: Replace with [MASK]\n    mask_indices = mask_arr & (torch.rand(labels.shape) < 0.8)\n    inputs[\"input_ids\"][mask_indices] = mask_token_id\n\n    # 10%: Replace with random token\n    random_indices = mask_arr & (torch.rand(labels.shape) < 0.1)\n    inputs[\"input_ids\"][random_indices] = torch.randint(0, vocab_size, labels.shape, dtype=torch.long)[random_indices]\n\n    # 10%: Keep original token (handled automatically by `labels`)\n\n    inputs[\"labels\"] = labels  # MLM Labels\n    inputs[\"attention_mask\"] = (inputs[\"input_ids\"] != tokenizer.pad_token_id).long()  # Padding Mask\n    return inputs\n\n\n# Tokenize the dataset\ntokenized_dataset = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\"],  # Remove the original text column\n)\n\n# Print sample tokenized output\nprint(tokenized_dataset[\"train\"][0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:43:11.255297Z","iopub.execute_input":"2025-03-28T13:43:11.255667Z","iopub.status.idle":"2025-03-28T13:47:34.386349Z","shell.execute_reply.started":"2025-03-28T13:43:11.255646Z","shell.execute_reply":"2025-03-28T13:47:34.385420Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24da80fdf50f4605aab3980b8613bc68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5384d5d71bc47338087a345377544c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1a73cf5b0c94d878ffb29b3d55f5587"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5d96937e0684f569d388a33ce2335b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04daa8831d6a451bae985f7fe1b498fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"024cd872062e42cf89dfbc21bb715a10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1801350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eedd8163cf74416a403fac662073bcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf576ddf2042448ab2c85bc1a17294f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2891 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5984c838bb904076a71cf44b7eabaa73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1165029 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528ac4a5f8bc49d6a66253884984709a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2461 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"914104d59c284e3ea7e86f2d91a07af7"}},"metadata":{}},{"name":"stdout","text":"{'input_ids': [0, 5457, 468, 44068, 6374, 41674, 6395, 50264, 50264, 50118, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 5457, 468, 44068, 6374, 41674, 6395, 5457, 1437, 50118, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"tokenized_dataset.save_to_disk(\"/kaggle/working/tokenized_roberta\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:47:34.387405Z","iopub.execute_input":"2025-03-28T13:47:34.387806Z","iopub.status.idle":"2025-03-28T13:47:36.853449Z","shell.execute_reply.started":"2025-03-28T13:47:34.387784Z","shell.execute_reply":"2025-03-28T13:47:36.852594Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/2891 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c25208b8482c4a1d96f62841f97ca8c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/4 shards):   0%|          | 0/1165029 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"787bee13ba9c4dbb91df00e10d812c00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/2461 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19fb17abf56d4e1aac62177f0736f34f"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Convert dataset to PyTorch tensors\nclass WikiTextDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.dataset[\"input_ids\"][idx]),\n            \"labels\": torch.tensor(self.dataset[\"labels\"][idx]),\n            \"attention_mask\": torch.tensor(self.dataset[\"attention_mask\"][idx]),\n        }\n\ntrain_dataset = WikiTextDataset(tokenized_dataset[\"train\"])\nvalid_dataset = WikiTextDataset(tokenized_dataset[\"validation\"])\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:47:36.854364Z","iopub.execute_input":"2025-03-28T13:47:36.854678Z","iopub.status.idle":"2025-03-28T13:49:53.143928Z","shell.execute_reply.started":"2025-03-28T13:47:36.854644Z","shell.execute_reply":"2025-03-28T13:49:53.143201Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#Roberta model Definition\n\nimport math\n\nclass RoBERTaEmbedding(nn.Module):\n    def __init__(self, vocab_size, embed_dim, max_len):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n        self.position_embedding = nn.Embedding(max_len, embed_dim)\n        self.layer_norm = nn.LayerNorm(embed_dim)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, input_ids):\n        seq_length = input_ids.shape[1]\n        position_ids = torch.arange(seq_length, device=input_ids.device).expand_as(input_ids)\n        embeddings = self.token_embedding(input_ids) + self.position_embedding(position_ids)\n        return self.dropout(self.layer_norm(embeddings))\n\nclass MultiheadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n        self.o_proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, x, attention_mask):\n        batch_size, seq_length, embed_dim = x.size()\n        qkv = self.qkv_proj(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n        q, k, v = torch.chunk(qkv, 3, dim=-1)\n\n        q, k, v = [tensor.transpose(1, 2) for tensor in (q, k, v)]\n\n        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        if attention_mask is not None:\n            scores = scores.masked_fill(attention_mask[:, None, None, :] == 0, float('-inf'))\n\n        attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n        attn_output = (attn_weights @ v).transpose(1, 2).reshape(batch_size, seq_length, embed_dim)\n\n        return self.o_proj(self.dropout(attn_output))\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, hidden_dim):\n        super().__init__()\n        self.attn = MultiheadSelfAttention(embed_dim, num_heads)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, embed_dim),\n            nn.Dropout(0.1)\n        )\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n    def forward(self, x, attention_mask):\n        x = self.norm1(x + self.attn(x, attention_mask))\n        x = self.norm2(x + self.ff(x))\n        return x\n\nclass RoBERTa(nn.Module):\n    def __init__(self, vocab_size, max_len, embed_dim=128, num_heads=4, hidden_dim=128, num_layers=4):\n        super().__init__()\n        self.embedding = RoBERTaEmbedding(vocab_size, embed_dim, max_len)\n        self.layers = nn.ModuleList([TransformerBlock(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)])\n        self.lm_head = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embedding(input_ids)\n        for layer in self.layers:\n            x = layer(x, attention_mask)\n        return self.lm_head(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:49:53.144772Z","iopub.execute_input":"2025-03-28T13:49:53.144984Z","iopub.status.idle":"2025-03-28T13:49:53.156795Z","shell.execute_reply.started":"2025-03-28T13:49:53.144966Z","shell.execute_reply":"2025-03-28T13:49:53.155931Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#Training Setup\n\nimport torch\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\nprint(\"Current Device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"CPU\")\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nvocab_size = 30522\nmax_len = 128\n\nmodel = RoBERTa(vocab_size, max_len).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer, num_warmup_steps=500, num_training_steps=len(train_loader) * 4  # Assuming 4 epochs\n)\n\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T13:49:53.157531Z","iopub.execute_input":"2025-03-28T13:49:53.157787Z","iopub.status.idle":"2025-03-28T13:50:59.454601Z","shell.execute_reply.started":"2025-03-28T13:49:53.157765Z","shell.execute_reply":"2025-03-28T13:50:59.453920Z"}},"outputs":[{"name":"stdout","text":"CUDA Available: True\nDevice Name: Tesla P100-PCIE-16GB\nCurrent Device: 0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"num_epochs = 4\ngrad_clip = 1.0  # Gradient clipping threshold\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n        print(\"sdgver\", fluch=True)\n        input_ids = batch[\"input_ids\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        optimizer.zero_grad()\n        \n        outputs = model(input_ids, attention_mask)\n        \n        loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  # Prevent exploding gradients\n        \n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.item()\n\n        # Print loss every 100 iterations\n        if (i + 1) % 100 == 0:\n            avg_loss = total_loss / (i + 1)\n            print(f\"Iteration {i+1}: Loss = {avg_loss:.4f}\")\n\n    print(f\"Epoch {epoch+1} completed. Average Loss = {total_loss / len(train_loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T14:35:11.701305Z","iopub.execute_input":"2025-03-28T14:35:11.701510Z","iopub.status.idle":"2025-03-28T14:35:11.783552Z","shell.execute_reply.started":"2025-03-28T14:35:11.701490Z","shell.execute_reply":"2025-03-28T14:35:11.782422Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c12375ba3342>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}